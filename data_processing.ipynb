{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep"
   ]
  },
  {
   "source": [
    "## 1. WalkHighlands"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_robots(url):\n",
    "    r = requests.get(url)\n",
    "    print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_HEADER = {'user-agent': 'Filip Balucha (s1913040@ed.ac.uk)'}\n",
    "URL_WALKHIGHLANDS = 'https://www.walkhighlands.co.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "User-agent: *\nDisallow: /admanage/\nDisallow: /sending/\n\nUser-agent: magpie-crawler\nDisallow: /\n"
     ]
    }
   ],
   "source": [
    "# Check robots.txt\n",
    "get_robots(URL_WALKHIGHLANDS + 'robots.txt')"
   ]
  },
  {
   "source": [
    "### Scrape the webpage with a list of most climbed munros"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Get HTML\n",
    "url = URL_WALKHIGHLANDS + 'munros/most-climbed'\n",
    "r = requests.get(url, headers=MY_HEADER)\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse response\n",
    "soup = BeautifulSoup(r.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the list of munros\n",
    "munros = []\n",
    "munro_elems = soup.findAll('tr')\n",
    "munro_elems = munro_elems\n",
    "header_indices = [0, 142]\n",
    "for i, munro_elem in enumerate(munro_elems):\n",
    "    if i in header_indices:  # ignore table headers\n",
    "        continue\n",
    "    name = str(munro_elem.a.contents[0])\n",
    "    href = munro_elem.a['href']\n",
    "    ascents = int(munro_elem.find_all('td')[-1].contents[0])\n",
    "    munros.append({\n",
    "        'name': name, \n",
    "        'ascent_count': ascents, \n",
    "        'href': href\n",
    "    })"
   ]
  },
  {
   "source": [
    "### Scrape individual munro subpages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Download subpage HTMLs to prevent repeated requests"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(munros):\n",
    "    for munro in munros:\n",
    "        name = munro[\"name\"]\n",
    "        print(f'Downloading: {name}')\n",
    "        url = URL_WALKHIGHLANDS + 'munros/' + munro['href']\n",
    "        r = requests.get(url, headers=MY_HEADER)\n",
    "        if r.status_code != 200:\n",
    "            print(f'Error: request to {url} returned status code {r.status_code}')\n",
    "            continue\n",
    "        # Store HTML in cache/\n",
    "        with open(f'cache/{name}.html', 'wb') as out:\n",
    "            out.write(r.content)\n",
    "            print('Success!\\n')\n",
    "        sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached = os.listdir('cache')\n",
    "to_download = [munro for munro in munros if f'{munro[\"name\"]}.html' not in cached]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "download(to_download)"
   ]
  },
  {
   "source": [
    "Determine accommodation types"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accom_hrefs(soup):\n",
    "    ACCOM_TEXT = 'Walker-friendly accommodation in the area'\n",
    "    accom_elem = soup.find(lambda tag: tag.contents and tag.contents[0] == ACCOM_TEXT)\n",
    "    current = accom_elem.next_sibling.next_sibling  # skip newline element\n",
    "    hrefs = []\n",
    "    while (current.name == 'p'):\n",
    "        href = current.a['href']\n",
    "        hrefs.append(href)\n",
    "        current = current.next_sibling\n",
    "    return hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_accom_type(href):\n",
    "    # The hrefs look like:\n",
    "    # /lochlomond/cottages_drymen.shtml -> cottages\n",
    "    # /lochlomond/hostels.shtml -> hostels\n",
    "    accom_type = href.split('.')[0]  # remove file suffix\n",
    "    accom_type = accom_type.split('/')  # remove subpage\n",
    "    accom_type = accom_type[2]\n",
    "    accom_type = accom_type.split('_')[0]\n",
    "    return accom_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Sgorr nam Fiannaidh (Aonach Eagach)']\n{'bedandbreakfast', 'cottages', 'hotels', 'hostels'}\n"
     ]
    }
   ],
   "source": [
    "accom_types = set()\n",
    "skipped = []  # munro subpages that could not be parsed\n",
    "for munro in munros:\n",
    "    name = munro['name']\n",
    "    html_file = f'./cache/{name}.html'\n",
    "    with open(html_file) as f:\n",
    "        contents = f.read()\n",
    "        soup = BeautifulSoup(contents, 'html')\n",
    "        try:\n",
    "            hrefs = get_accom_hrefs(soup)\n",
    "            accom_types.update(map(_get_accom_type, hrefs))\n",
    "        except:\n",
    "            skipped.append(name)\n",
    " \n",
    "print(skipped)\n",
    "print(accom_types)"
   ]
  },
  {
   "source": [
    "Parse munro subpages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract details from a response object\n",
    "def _extract_munro_details(soup):\n",
    "    details = {}\n",
    "    # Extract rating\n",
    "    rating_str = soup.find('strong', itemprop='ratingValue').contents[0]\n",
    "    rating = float(rating_str)\n",
    "    details['rating'] = rating\n",
    "    # Extract altitude\n",
    "    altitude_str = soup.findAll('p')[5].contents[0]\n",
    "    altitude = int(re.sub(r'\\D', '', altitude_str))  # extract integer from string\n",
    "    details['altitude'] = altitude\n",
    "    # Extract accommodation count # TODO\n",
    "    # accom_count = _get_accom_count(soup)\n",
    "    # details['accom_count'] = accom_count\n",
    "    \n",
    "    return details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_munro_subpage(url):\n",
    "    r = requests.get(url, headers=MY_HEADER)\n",
    "    if r.status_code != 200:\n",
    "        print(f'Error: request to {url} returned status code {r.status_code}')\n",
    "        return\n",
    "    # Parse response\n",
    "    details = _extract_munro_details(r)\n",
    "    return details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accommodation hrefs\n",
    "accom_text = 'Walker-friendly accommodation in the area'\n",
    "accom_elem = soup.find(lambda tag: tag.contents and tag.contents[0] == accom_text)\n",
    "current = accom_elem.next_sibling.next_sibling  # skip newline element\n",
    "hrefs = []\n",
    "while (current.name == 'p'):\n",
    "    href = current.a['href']\n",
    "    hrefs.append(href)\n",
    "    current = current.next_sibling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up cache and download accommodation HTMLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for munro in munros:\n",
    "    name = munro['name']\n",
    "    html_file = f'./cache/{name}.html'\n",
    "    with open(html_file) as f:\n",
    "        contents = f.read()\n",
    "        soup = BeautifulSoup(contents, 'html')\n",
    "        details = _extract_munro_details(soup)\n",
    "        munro.update(details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           name  ascent_count          href  rating  altitude\n",
       "0    Ben Lomond         20531    ben-lomond     3.8       974\n",
       "1     Ben Nevis         17892     ben-nevis     3.9      1345\n",
       "2    Ben Lawers         16063    ben-lawers     3.9      1214\n",
       "3  Schiehallion         15924  schiehallion     3.6      1083\n",
       "4   Beinn Ghlas         15646   beinn-ghlas     3.4      1103"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>ascent_count</th>\n      <th>href</th>\n      <th>rating</th>\n      <th>altitude</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ben Lomond</td>\n      <td>20531</td>\n      <td>ben-lomond</td>\n      <td>3.8</td>\n      <td>974</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Ben Nevis</td>\n      <td>17892</td>\n      <td>ben-nevis</td>\n      <td>3.9</td>\n      <td>1345</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ben Lawers</td>\n      <td>16063</td>\n      <td>ben-lawers</td>\n      <td>3.9</td>\n      <td>1214</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Schiehallion</td>\n      <td>15924</td>\n      <td>schiehallion</td>\n      <td>3.6</td>\n      <td>1083</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Beinn Ghlas</td>\n      <td>15646</td>\n      <td>beinn-ghlas</td>\n      <td>3.4</td>\n      <td>1103</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(munros)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('datasets/clean_v1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# URGENT\n",
    "# Go through each HTML\n",
    "    # Parse HTML for to get altitude and rating\n",
    "    # -> df -> CSV\n",
    "\n",
    "# LESS URGENT\n",
    "# Create a cache: accom_hyperlink -> accom_count\n",
    "# Go through each HTML\n",
    "    # Parse HTML for to get accomodation hyperlinks\n",
    "        # For each hyperlink, try to fetch data from cache, else compute and update cache\n",
    "        # Note: Sgorr nam Fiannaidh (Aonach Eagach) caused trouble, so make sure that works properly (weird header span)\n",
    "        # If there is no hyperlink for some accommodation, store 0\n",
    "\n",
    "# FOR LATER\n",
    "# Join with 2nd DB"
   ]
  }
 ]
}