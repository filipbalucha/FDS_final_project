{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep"
   ]
  },
  {
   "source": [
    "# 1. WalkHighlands"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_robots(url):\n",
    "    r = requests.get(url)\n",
    "    print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_HEADER = {'user-agent': 'Filip Balucha (s1913040@ed.ac.uk)'}\n",
    "URL_WALKHIGHLANDS = 'https://www.walkhighlands.co.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "User-agent: *\nDisallow: /admanage/\nDisallow: /sending/\n\nUser-agent: magpie-crawler\nDisallow: /\n"
     ]
    }
   ],
   "source": [
    "# Check robots.txt\n",
    "get_robots(URL_WALKHIGHLANDS + 'robots.txt')"
   ]
  },
  {
   "source": [
    "## Scrape the webpage with a list of most climbed munros"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Get HTML\n",
    "url = URL_WALKHIGHLANDS + 'munros/most-climbed'\n",
    "r = requests.get(url, headers=MY_HEADER)\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse response\n",
    "soup = BeautifulSoup(r.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the list of munros\n",
    "munros = []\n",
    "munro_elems = soup.findAll('tr')\n",
    "header_indices = [0, 142]\n",
    "for i, munro_elem in enumerate(munro_elems):\n",
    "    if i in header_indices:  # ignore table headers\n",
    "        continue\n",
    "    name = str(munro_elem.a.contents[0])\n",
    "    href = munro_elem.a['href']\n",
    "    ascents = int(munro_elem.find_all('td')[-1].contents[0])\n",
    "    munros.append({\n",
    "        'name': name, \n",
    "        'ascent_count': ascents, \n",
    "        'href': href\n",
    "    })"
   ]
  },
  {
   "source": [
    "### Scrape munro subpages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1. Download subpage HTMLs to prevent repeated requests"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(munros):\n",
    "    for munro in munros:\n",
    "        name = munro[\"name\"]\n",
    "        print(f'Downloading: {name}')\n",
    "        url = URL_WALKHIGHLANDS + 'munros/' + munro['href']\n",
    "        r = requests.get(url, headers=MY_HEADER)\n",
    "        if r.status_code != 200:\n",
    "            print(f'Error: request to {url} returned status code {r.status_code}')\n",
    "            continue\n",
    "        # Store HTML in cache\n",
    "        with open(f'cache/munros/{name}.html', 'wb') as out:\n",
    "            out.write(r.content)\n",
    "            print('Success!\\n')\n",
    "        sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached = os.listdir('cache/munros')\n",
    "to_download = [munro for munro in munros if f'{munro[\"name\"]}.html' not in cached]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "download(to_download)"
   ]
  },
  {
   "source": [
    "### 2. Extract details from munro subpages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_accom_type(accom_href):\n",
    "    # Extract accommodation type from accommodation href, e.g.:\n",
    "        # /lochlomond/cottages_drymen.shtml -> cottages\n",
    "        # /lochlomond/hostels.shtml -> hostels\n",
    "    accom_type = accom_href.split('.')[0]  # ignore suffix\n",
    "    accom_type = accom_type.split('/')[2]  # ignore subpage\n",
    "    accom_type = accom_type.split('_')[0]  # ignore geographical tag\n",
    "    return accom_type\n",
    "\n",
    "def _get_accom_hrefs(soup):\n",
    "    ACCOM_TEXT = 'Walker-friendly accommodation in the area'\n",
    "    accom_elem = soup.find(lambda tag: tag.contents and tag.contents[0] == ACCOM_TEXT)\n",
    "    current = accom_elem.next_sibling.next_sibling  # skip newline element\n",
    "    accom_hrefs = {}\n",
    "    while (current.name == 'p'):  # accom hrefs are stored in p tags\n",
    "        href = current.a['href']\n",
    "        accom_type = _get_accom_type(href)\n",
    "        accom_hrefs['href_'+accom_type] = href\n",
    "        current = current.next_sibling\n",
    "    return accom_hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_munro_details(soup):\n",
    "    details = {}\n",
    "    # Extract rating\n",
    "    rating_str = soup.find('strong', itemprop='ratingValue').contents[0]\n",
    "    rating = float(rating_str)\n",
    "    details['rating'] = rating\n",
    "    # Extract altitude\n",
    "    altitude_str = soup.findAll('p')[5].contents[0]\n",
    "    altitude = int(re.sub(r'\\D', '', altitude_str))  # extract integer from string\n",
    "    details['altitude'] = altitude\n",
    "    # Extract accommodation hrefs\n",
    "    try:\n",
    "        accom_hrefs = _get_accom_hrefs(soup)\n",
    "        munro.update(accom_hrefs)\n",
    "    except:  # subpage could not be parsed\n",
    "        print(f'Skipping {name}')\n",
    "    # TODO: extract accommodation count\n",
    "    \n",
    "    return details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_munro_subpage(url):\n",
    "    r = requests.get(url, headers=MY_HEADER)\n",
    "    if r.status_code != 200:\n",
    "        print(f'Error: request to {url} returned status code {r.status_code}')\n",
    "        return\n",
    "    # Parse response\n",
    "    details = _extract_munro_details(r)\n",
    "    return details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Skipping Sgorr nam Fiannaidh (Aonach Eagach)\n"
     ]
    }
   ],
   "source": [
    "for munro in munros:\n",
    "    name = munro['name']\n",
    "    html_file = f'./cache/munros/{name}.html'\n",
    "    with open(html_file) as f:\n",
    "        contents = f.read()\n",
    "        soup = BeautifulSoup(contents, 'html')\n",
    "        details = _extract_munro_details(soup)\n",
    "        munro.update(details)"
   ]
  },
  {
   "source": [
    "### 3. Handle munros with a faulty HTML"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "faulty_munro_name = 'Sgorr nam Fiannaidh'\n",
    "for munro in munros:\n",
    "    if munro['name'] == faulty_munro_name:\n",
    "        munro['href_hotels']: '/fortwilliam/hotels_glencoe.shtml' \n",
    "        munro['href_bedandbreakfast']: '/fortwilliam/bedandbreakfast_glencoe.shtml' \n",
    "        munro['href_cottages']: '/fortwilliam/cottages_glencoe.shtml'\n",
    "        munro['href_hostels']: '/fortwilliam/hostels_glencoe.shtml'\n",
    "        break"
   ]
  },
  {
   "source": [
    "### 4. Export dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas dataframe\n",
    "df = pd.DataFrame.from_dict(munros)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataset\n",
    "df_out = df[['name', 'altitude', 'ascent_count', 'rating']]\n",
    "df_out.to_csv('datasets/clean_v1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# 1. Get accom. count for each accom. type\n",
    "    # Download accommodation subpages\n",
    "# 2. Join with 2nd DB"
   ]
  }
 ]
}