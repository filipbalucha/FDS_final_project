{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep"
   ]
  },
  {
   "source": [
    "## 1. WalkHighlands"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_robots(url):\n",
    "    r = requests.get(url)\n",
    "    print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_HEADER = {'user-agent': 'Filip Balucha (s1913040@ed.ac.uk)'}\n",
    "URL_WALKHIGHLANDS = 'https://www.walkhighlands.co.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "User-agent: *\nDisallow: /admanage/\nDisallow: /sending/\n\nUser-agent: magpie-crawler\nDisallow: /\n"
     ]
    }
   ],
   "source": [
    "# Check robots.txt\n",
    "get_robots(URL_WALKHIGHLANDS + 'robots.txt')"
   ]
  },
  {
   "source": [
    "### Scrape the webpage with a list of most climbed munros"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "metadata": {},
     "execution_count": 520
    }
   ],
   "source": [
    "# Get HTML\n",
    "url = URL_WALKHIGHLANDS + 'munros/most-climbed'\n",
    "r = requests.get(url, headers=MY_HEADER)\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse response\n",
    "soup = BeautifulSoup(r.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the list of munros\n",
    "munros = []\n",
    "munro_elems = soup.findAll('tr')\n",
    "munro_elems = munro_elems\n",
    "header_indices = [0, 142]\n",
    "for i, munro_elem in enumerate(munro_elems):\n",
    "    if i in header_indices:  # ignore table headers\n",
    "        continue\n",
    "    name = str(munro_elem.a.contents[0])\n",
    "    href = munro_elem.a['href']\n",
    "    ascents = int(munro_elem.find_all('td')[-1].contents[0])\n",
    "    munros.append({\n",
    "        'name': name, \n",
    "        'ascent_count': ascents, \n",
    "        'href': href\n",
    "    })"
   ]
  },
  {
   "source": [
    "### Scrape individual munro subpages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Download subpage HTMLs to prevent repeated requests"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(munros):\n",
    "    for munro in munros:\n",
    "        name = munro[\"name\"]\n",
    "        print(f'Downloading: {name}')\n",
    "        url = URL_WALKHIGHLANDS + 'munros/' + munro['href']\n",
    "        r = requests.get(url, headers=MY_HEADER)\n",
    "        if r.status_code != 200:\n",
    "            print(f'Error: request to {url} returned status code {r.status_code}')\n",
    "            continue\n",
    "        # Store HTML in cache/\n",
    "        with open(f'cache/{name}.html', 'wb') as out:\n",
    "            out.write(r.content)\n",
    "            print('Success!\\n')\n",
    "        sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached = os.listdir('cache')\n",
    "to_download = [munro for munro in munros if f'{munro[\"name\"]}.html' not in cached]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "download(to_download)"
   ]
  },
  {
   "source": [
    "Add helper methods to help parsing munro subpages:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_munro_subpage(url):\n",
    "    r = requests.get(url, headers=MY_HEADER)\n",
    "    if r.status_code != 200:\n",
    "        print(f'Error: request to {url} returned status code {r.status_code}')\n",
    "        return\n",
    "    # Parse response\n",
    "    details = _extract_munro_details(r)\n",
    "    return details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract details from a response object\n",
    "def _extract_munro_details(r):\n",
    "    # Parse response\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    details = {}\n",
    "    # Extract rating\n",
    "    rating_str = soup.find('strong', itemprop='ratingValue').contents[0]\n",
    "    rating = float(rating_str)\n",
    "    details['rating'] = rating\n",
    "    # Extract altitude\n",
    "    altitude_str = soup.findAll('p')[5].contents[0]\n",
    "    altitude = int(re.sub(r'\\D', '', altitude_str))  # extract integer from string\n",
    "    details['altitude'] = altitude\n",
    "    # Extract accommodation count\n",
    "    accom_count = _get_accom_count(soup)\n",
    "    details['accom_count'] = accom_count\n",
    "    \n",
    "    return details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accommodation count\n",
    "def _get_accom_count(soup):\n",
    "    # Accommodation list is bounded by two headers:\n",
    "    current = soup.findAll('h3')[1]\n",
    "    end = soup.findAll('h3')[2]\n",
    "    accom_count = 0\n",
    "    # Walk the list between the two headers\n",
    "    while current is not end:\n",
    "        if current.name == 'p':  # hotels are stored in paragraph tags\n",
    "            accom_count += 1\n",
    "        current = current.next_sibling  # move to next element\n",
    "    return accom_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for munro in munros:\n",
    "#     print(f'Request for: {munro[\"name\"]}')\n",
    "#     url = URL_WALKHIGHLANDS + 'munros/' + munro['href']\n",
    "#     details = parse_munro_subpage(url)\n",
    "#     munro.update(details)  # add new details to munro\n",
    "#     print('Success!')\n",
    "#     sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# URGENT\n",
    "# Go through each HTML\n",
    "    # Parse HTML for to get altitude and rating\n",
    "    # -> df -> CSV\n",
    "\n",
    "# LESS URGENT\n",
    "# Create a cache: accom_hyperlink -> accom_count\n",
    "# Go through each HTML\n",
    "    # Parse HTML for to get accomodation hyperlinks\n",
    "        # For each hyperlink, try to fetch data from cache, else compute and update cache\n",
    "        # Note: Sgorr nam Fiannaidh (Aonach Eagach) caused trouble, so make sure that works properly (weird header span)\n",
    "        # If there is no hyperlink for some accommodation, store 0\n",
    "\n",
    "# FOR LATER\n",
    "# Join with 2nd DB"
   ]
  }
 ]
}